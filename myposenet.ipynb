{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "Cloning into 'examples'...\n",
      "Updating files:  15% (438/2850)\n",
      "Updating files:  16% (456/2850)\n",
      "Updating files:  17% (485/2850)\n",
      "Updating files:  18% (513/2850)\n",
      "Updating files:  19% (542/2850)\n",
      "Updating files:  20% (570/2850)\n",
      "Updating files:  21% (599/2850)\n",
      "Updating files:  22% (627/2850)\n",
      "Updating files:  23% (656/2850)\n",
      "Updating files:  24% (684/2850)\n",
      "Updating files:  25% (713/2850)\n",
      "Updating files:  26% (741/2850)\n",
      "Updating files:  26% (749/2850)\n",
      "Updating files:  27% (770/2850)\n",
      "Updating files:  28% (798/2850)\n",
      "Updating files:  29% (827/2850)\n",
      "Updating files:  30% (855/2850)\n",
      "Updating files:  31% (884/2850)\n",
      "Updating files:  32% (912/2850)\n",
      "Updating files:  33% (941/2850)\n",
      "Updating files:  34% (969/2850)\n",
      "Updating files:  35% (998/2850)\n",
      "Updating files:  36% (1026/2850)\n",
      "Updating files:  37% (1055/2850)\n",
      "Updating files:  38% (1083/2850)\n",
      "Updating files:  39% (1112/2850)\n",
      "Updating files:  39% (1123/2850)\n",
      "Updating files:  40% (1140/2850)\n",
      "Updating files:  41% (1169/2850)\n",
      "Updating files:  42% (1197/2850)\n",
      "Updating files:  43% (1226/2850)\n",
      "Updating files:  44% (1254/2850)\n",
      "Updating files:  45% (1283/2850)\n",
      "Updating files:  46% (1311/2850)\n",
      "Updating files:  47% (1340/2850)\n",
      "Updating files:  47% (1351/2850)\n",
      "Updating files:  48% (1368/2850)\n",
      "Updating files:  49% (1397/2850)\n",
      "Updating files:  50% (1425/2850)\n",
      "Updating files:  51% (1454/2850)\n",
      "Updating files:  52% (1482/2850)\n",
      "Updating files:  53% (1511/2850)\n",
      "Updating files:  54% (1539/2850)\n",
      "Updating files:  55% (1568/2850)\n",
      "Updating files:  55% (1570/2850)\n",
      "Updating files:  56% (1596/2850)\n",
      "Updating files:  57% (1625/2850)\n",
      "Updating files:  58% (1653/2850)\n",
      "Updating files:  59% (1682/2850)\n",
      "Updating files:  60% (1710/2850)\n",
      "Updating files:  60% (1738/2850)\n",
      "Updating files:  61% (1739/2850)\n",
      "Updating files:  62% (1767/2850)\n",
      "Updating files:  63% (1796/2850)\n",
      "Updating files:  64% (1824/2850)\n",
      "Updating files:  65% (1853/2850)\n",
      "Updating files:  66% (1881/2850)\n",
      "Updating files:  67% (1910/2850)\n",
      "Updating files:  68% (1938/2850)\n",
      "Updating files:  68% (1950/2850)\n",
      "Updating files:  69% (1967/2850)\n",
      "Updating files:  70% (1995/2850)\n",
      "Updating files:  71% (2024/2850)\n",
      "Updating files:  72% (2052/2850)\n",
      "Updating files:  73% (2081/2850)\n",
      "Updating files:  74% (2109/2850)\n",
      "Updating files:  75% (2138/2850)\n",
      "Updating files:  76% (2166/2850)\n",
      "Updating files:  76% (2181/2850)\n",
      "Updating files:  77% (2195/2850)\n",
      "Updating files:  78% (2223/2850)\n",
      "Updating files:  79% (2252/2850)\n",
      "Updating files:  80% (2280/2850)\n",
      "Updating files:  81% (2309/2850)\n",
      "Updating files:  82% (2337/2850)\n",
      "Updating files:  83% (2366/2850)\n",
      "Updating files:  84% (2394/2850)\n",
      "Updating files:  85% (2423/2850)\n",
      "Updating files:  86% (2451/2850)\n",
      "Updating files:  86% (2474/2850)\n",
      "Updating files:  87% (2480/2850)\n",
      "Updating files:  88% (2508/2850)\n",
      "Updating files:  89% (2537/2850)\n",
      "Updating files:  90% (2565/2850)\n",
      "Updating files:  91% (2594/2850)\n",
      "Updating files:  92% (2622/2850)\n",
      "Updating files:  93% (2651/2850)\n",
      "Updating files:  94% (2679/2850)\n",
      "Updating files:  95% (2708/2850)\n",
      "Updating files:  96% (2736/2850)\n",
      "Updating files:  97% (2765/2850)\n",
      "Updating files:  98% (2793/2850)\n",
      "Updating files:  99% (2822/2850)\n",
      "Updating files: 100% (2850/2850)\n",
      "Updating files: 100% (2850/2850), done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not open 'movenet_thunder.tflite'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YYY\\Desktop\\Project\\myposenet.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m BodyPart\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m Movenet\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m movenet \u001b[39m=\u001b[39m Movenet(\u001b[39m'\u001b[39;49m\u001b[39mmovenet_thunder\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Define function to run pose estimation using MoveNet Thunder.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# You'll apply MoveNet's cropping algorithm and run inference multiple times on\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# the input image to improve pose estimation accuracy.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(input_tensor, inference_count\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\Project\\examples/lite/examples/pose_estimation/raspberry_pi\\ml\\movenet.py:58\u001b[0m, in \u001b[0;36mMovenet.__init__\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     55\u001b[0m   model_name \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.tflite\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[39m# Initialize model\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m interpreter \u001b[39m=\u001b[39m Interpreter(model_path\u001b[39m=\u001b[39;49mmodel_name, num_threads\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m interpreter\u001b[39m.\u001b[39mallocate_tensors()\n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_index \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_input_details()[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\lite\\python\\interpreter.py:455\u001b[0m, in \u001b[0;36mInterpreter.__init__\u001b[1;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[0;32m    448\u001b[0m custom_op_registerers_by_name \u001b[39m=\u001b[39m [\n\u001b[0;32m    449\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    450\u001b[0m ]\n\u001b[0;32m    451\u001b[0m custom_op_registerers_by_func \u001b[39m=\u001b[39m [\n\u001b[0;32m    452\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    453\u001b[0m ]\n\u001b[0;32m    454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 455\u001b[0m     _interpreter_wrapper\u001b[39m.\u001b[39;49mCreateWrapperFromFile(\n\u001b[0;32m    456\u001b[0m         model_path, op_resolver_id, custom_op_registerers_by_name,\n\u001b[0;32m    457\u001b[0m         custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter:\n\u001b[0;32m    459\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFailed to open \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model_path))\n",
      "\u001b[1;31mValueError\u001b[0m: Could not open 'movenet_thunder.tflite'."
     ]
    }
   ],
   "source": [
    "#@title Functions to run pose estimation with MoveNet\n",
    "\n",
    "#@markdown You'll download the MoveNet Thunder model from [TensorFlow Hub](https://www.google.com/url?sa=D&q=https%3A%2F%2Ftfhub.dev%2Fs%3Fq%3Dmovenet), and reuse some inference and visualization logic from the [MoveNet Raspberry Pi (Python)](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi) sample app to detect landmarks (ear, nose, wrist etc.) from the input images.\n",
    "\n",
    "#@markdown *Note: You should use the most accurate pose estimation model (i.e. MoveNet Thunder) to detect the keypoints and use them to train the pose classification model to achieve the best accuracy. When running inference, you can use a pose estimation model of your choice (e.g. either MoveNet Lightning or Thunder).*\n",
    "\n",
    "# Download model from TF Hub and check out inference code from GitHub\n",
    "!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "!git clone https://github.com/tensorflow/examples.git\n",
    "pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\n",
    "sys.path.append(pose_sample_rpi_path)\n",
    "\n",
    "# Load MoveNet Thunder model\n",
    "import utils\n",
    "from data import BodyPart\n",
    "from ml import Movenet\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "# Define function to run pose estimation using MoveNet Thunder.\n",
    "# You'll apply MoveNet's cropping algorithm and run inference multiple times on\n",
    "# the input image to improve pose estimation accuracy.\n",
    "def detect(input_tensor, inference_count=3):\n",
    "  \"\"\"Runs detection on an input image.\n",
    " \n",
    "  Args:\n",
    "    input_tensor: A [height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "    inference_count: Number of times the model should run repeatly on the\n",
    "      same input image to improve detection accuracy.\n",
    " \n",
    "  Returns:\n",
    "    A Person entity detected by the MoveNet.SinglePose.\n",
    "  \"\"\"\n",
    "  image_height, image_width, channel = input_tensor.shape\n",
    " \n",
    "  # Detect pose using the full input image\n",
    "  movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    " \n",
    "  # Repeatedly using previous detection result to identify the region of\n",
    "  # interest and only croping that region to improve detection accuracy\n",
    "  for _ in range(inference_count - 1):\n",
    "    person = movenet.detect(input_tensor.numpy(), \n",
    "                            reset_crop_region=False)\n",
    "\n",
    "  return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import utils\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a video capture object \n",
    "vid = cv2.VideoCapture(0) \n",
    "  \n",
    "while(True): \n",
    "      \n",
    "    # Capture the video frame \n",
    "    # by frame \n",
    "    ret, frame = vid.read() \n",
    "  \n",
    "    # Display the resulting frame \n",
    "    cv2.imshow('frame', frame) \n",
    "      \n",
    "    # the 'q' button is set as the \n",
    "    # quitting button you may use any \n",
    "    # desired button of your choice \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lite-model_movenet_singlepose_thunder_tflite_float16_4.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "def movenet(input_image):\n",
    "  \"\"\"Runs detection on an input image.\n",
    "\n",
    "  Args:\n",
    "    input_image: A [1, height, width, 3] tensor represents the input image\n",
    "      pixels. Note that the height/width should already be resized and match the\n",
    "      expected input resolution of the model before passing into this function.\n",
    "\n",
    "  Returns:\n",
    "    A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "    coordinates and scores.\n",
    "  \"\"\"\n",
    "  # TF Lite format expects tensor type of uint8.\n",
    "  input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "  # Invoke inference.\n",
    "  interpreter.invoke()\n",
    "  # Get the model prediction.\n",
    "  keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image(\n",
    "    image, person, crop_region=None, close_figure=True,\n",
    "    keep_input_size=False):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    " \n",
    "  Args:\n",
    "    image: An numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    person: A person entity returned from the MoveNet.SinglePose model.\n",
    "    close_figure: Whether to close the plt figure after the function returns.\n",
    "    keep_input_size: Whether to keep the size of the input image.\n",
    " \n",
    "  Returns:\n",
    "    An numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  # Draw the detection result on top of the image.\n",
    "  image_np = utils.visualize(image, [person])\n",
    "  \n",
    "  # Plot the image with detection results.\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  im = ax.imshow(image_np)\n",
    " \n",
    "  if close_figure:\n",
    "    plt.close(fig)\n",
    " \n",
    "  if not keep_input_size:\n",
    "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
    "\n",
    "  return image_np\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image.\n",
    "image_path = 'person.jpg' # png bmp jpeg\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.image.decode_jpeg(image)\n",
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "print\n",
    "\n",
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(input_image) # [17, 3]\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'visualize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YYY\\Desktop\\Project\\myposenet.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m display_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize_with_pad(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     display_image, \u001b[39m1280\u001b[39m, \u001b[39m1280\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m output_overlay \u001b[39m=\u001b[39m draw_prediction_on_image(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     np\u001b[39m.\u001b[39;49msqueeze(display_image\u001b[39m.\u001b[39;49mnumpy(), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), keypoints_with_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(output_overlay)\n",
      "\u001b[1;32mc:\\Users\\YYY\\Desktop\\Project\\myposenet.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"Draws the keypoint predictions on image.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m  image overlaid with keypoint predictions.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Draw the detection result on top of the image.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m image_np \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mvisualize(image, [person])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Plot the image with detection results.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YYY/Desktop/Project/myposenet.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m height, width, channel \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'visualize'"
     ]
    }
   ],
   "source": [
    "# Visualize the predictions with image.\n",
    "display_image = tf.expand_dims(image, axis=0)\n",
    "display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    display_image, 1280, 1280), dtype=tf.int32)\n",
    "output_overlay = draw_prediction_on_image(\n",
    "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(output_overlay)\n",
    "_ = plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
